# -*- coding: utf-8 -*-
"""dmg ass2_xg_boost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bsmUTEws8nZtkEYvoEVc0AJvM3eB9OuM
"""

cd ./drive/My Drive/dmg ass2

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier

from sklearn.model_selection import cross_val_score  #doubt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

import xgboost as xgb

import pickle 
import seaborn as sns




def runner(dataset_test_X_main):
    dataset= pd.read_csv("given_dataset.csv") #dataset is a dataframe
    df0 =dataset[dataset['T']==0 ]
    df1 =dataset[dataset['T']==1 ]


    X_cols=dataset.columns.values[:-1]
    Y_cols=dataset.columns.values[-1:]



    ratio=1.0
    split=int(ratio*dataset.shape[0])

    X_train=dataset[X_cols][:split]
    Y_train=dataset[Y_cols][:split]

    #X_test=dataset[X_cols][split:]
    #Y_test=dataset[Y_cols][split:]


    dataset['T'].value_counts().plot(kind='bar');
    dataset['T'].value_counts()




    dataset_test_X=pd.read_csv(dataset_test_X_main)
    dataset_test_X=dataset_test_X.to_numpy()


    prediction_list=[]
    train_prediction_list=[]


    for i in range(60): # 200 times adaboost    


        random_df= df0
        random_df=random_df.append( df1.sample(frac = 0.018) )

        random_df=random_df.sample(frac=1)
        X_training_sample=random_df[X_cols][0:]
        Y_training_sample=random_df[Y_cols][0:]

        X_training_sample=X_training_sample.to_numpy()
        Y_training_sample=Y_training_sample['T']

        xgb_model = xgb.XGBClassifier( n_estimators=200, max_depth=8)


        xgb_model.fit(X_training_sample,Y_training_sample)
        Y_predictedValue = xgb_model.predict(dataset_test_X)
        prediction_list.append(Y_predictedValue)


        Y_train_predictedValue = xgb_model.predict(X_training_sample)
        train_prediction_list.append(Y_train_predictedValue)
        accuracy=accuracy_score(Y_training_sample,Y_train_predictedValue)

        print("training acc->",accuracy)
        print(i+1,"  is done ,", 60-i-1,"is left")





    random_df['T'].value_counts()
    random_df['T'].value_counts().plot(kind='bar');



    import numpy as np

    prediction_list=np.array(prediction_list)
    print(prediction_list)
    print(prediction_list.shape)


    np.save('pred_npy_xg_boost.npy',prediction_list)#saving the model 60 times was not possible thus saved the .npy file


    from scipy import stats

    m = stats.mode(prediction_list)
    print(m)


    n = stats.mode(train_prediction_list)
    print(n)


    Y_train_predictedValue_final=n[0][0]
    accuracy=accuracy_score(Y_training_sample,Y_train_predictedValue_final)
    print("finnal training acc->",accuracy)

    #Y_predictedValue_testData=Y_predictedValue

    Y_predictedValue_testData=m[0][0]
    data={'id':[],'T':[]}
    df=pd.DataFrame(data)


    dataset_final = pd.DataFrame({'T': Y_predictedValue_testData[0:,]})
    print(dataset_final)
    dataset2_test_X= pd.read_csv("to_predict.csv")
    # Place the DataFrames side by side
    horizontal_stack = pd.concat([dataset2_test_X['id'],dataset_final], axis=1)
    print(horizontal_stack)

    horizontal_stack.to_csv('xg_boost_file1.csv',index=False)




dataset_test_X_main= "to_predict.csv"
runner(dataset_test_X_main)